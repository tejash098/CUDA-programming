{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MM65jneGtYIj",
        "outputId": "6c74200a-c29c-4c3c-9854-eb6104410445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-1f3e5gph\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-1f3e5gph\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 781ff5b76ba6c4c2d80dcbbec9983e147613cc71\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: nvcc4jupyter\n",
            "  Building wheel for nvcc4jupyter (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvcc4jupyter: filename=nvcc4jupyter-1.1.0-py3-none-any.whl size=8011 sha256=e2b2acbb79d726227ace2ffdeb18b901e4debb9cf9419b13a35a389a655ee4ac\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-i8hclcq1/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\n",
            "Successfully built nvcc4jupyter\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CcKQg3AtemK",
        "outputId": "31b7eeb7-dd70-484c-b8dc-ecac7c86d4c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source files will be saved in \"/tmp/tmpxoiaf_6z\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "write a program to sum of two numbers using cuda"
      ],
      "metadata": {
        "id": "5aXy18AhmKeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "__global__ void add(int *a, int *b, int *c) {\n",
        "*c = *a + *b;\n",
        "}\n",
        "int main() {\n",
        "int a, b, c;\n",
        "int *d_a, *d_b, *d_c;\n",
        "int size = sizeof(int);\n",
        "cudaMalloc((void **)&d_a, size);\n",
        "cudaMalloc((void **)&d_b, size);\n",
        "cudaMalloc((void **)&d_c, size);\n",
        "c = 0;\n",
        "a = 4;\n",
        "b = 5;\n",
        "\n",
        "cudaMemcpy(d_a, &a, size, cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(d_b, &b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "add<<<1,1>>>(d_a, d_b, d_c);\n",
        "\n",
        "cudaError err = cudaMemcpy(&c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "  if(err!=cudaSuccess) {\n",
        "      printf(\"CUDA error copying to Host: %s\\n\", cudaGetErrorString(err));\n",
        "  }\n",
        "printf(\"result is %d\\n\",c);\n",
        "\n",
        "cudaFree(d_a);\n",
        "cudaFree(d_b);\n",
        "cudaFree(d_c);\n",
        "return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NS6vbb-steu8",
        "outputId": "bdef6a33-ced3-4049-bd26-d86c24a7e9c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "result is 9\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a program to multiply square matrices using CUDA.(N*N)"
      ],
      "metadata": {
        "id": "Mr0HOwpXmS6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "\n",
        "#define N 2 // size of the matrix (N x N)\n",
        "\n",
        "__global__ void matrixMul(int *a, int *b, int *c) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    int temp_sum = 0;\n",
        "    if (row < N && col < N) {\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            temp_sum += a[row * N + i] * b[i * N + col];\n",
        "        }\n",
        "        c[row * N + col] = temp_sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int size = N * N * sizeof(int);\n",
        "    int a[N*N] = {1, 2, 3, 4};\n",
        "    int b[N*N] = {16, 15, 14, 13};\n",
        "    int c[N*N];\n",
        "    int *d_a, *d_b, *d_c;\n",
        "\n",
        "    // allocate memory on the device\n",
        "    cudaMalloc((void **)&d_a, size);\n",
        "    cudaMalloc((void **)&d_b, size);\n",
        "    cudaMalloc((void **)&d_c, size);\n",
        "\n",
        "    // copy inputs to device\n",
        "    cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // launch the kernel\n",
        "    dim3 threadsPerBlock(N, N);\n",
        "    dim3 blocksPerGrid(1, 1);\n",
        "    if (N*N > 512){\n",
        "        threadsPerBlock.x = 512;\n",
        "        threadsPerBlock.y = 512;\n",
        "        blocksPerGrid.x = ceil(double(N)/double(threadsPerBlock.x));\n",
        "        blocksPerGrid.y = ceil(double(N)/double(threadsPerBlock.y));\n",
        "    }\n",
        "\n",
        "    matrixMul<<<blocksPerGrid,threadsPerBlock>>>(d_a, d_b, d_c);\n",
        "\n",
        "    // copy result back to host\n",
        "    cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // print the result\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        for (int j = 0; j < N; j++) {\n",
        "            printf(\"%d \", c[i * N + j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    // free memory on the device\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qebmkPwqte41",
        "outputId": "18dc9de9-21c7-4062-bab2-517d0104f07a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44 41 \n",
            "104 97 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "\n",
        "#define N 4 // size of the matrix (N x N)\n",
        "\n",
        "__global__ void matrixMul(int *a, int *b, int *c) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    int temp_sum = 0;\n",
        "    if (row < N && col < N) {\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            temp_sum += a[row * N + i] * b[i * N + col];\n",
        "        }\n",
        "        c[row * N + col] = temp_sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int size = N * N * sizeof(int);\n",
        "    int a[N*N] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16};\n",
        "    int b[N*N] = {16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1};\n",
        "    int c[N*N];\n",
        "    int *d_a, *d_b, *d_c;\n",
        "\n",
        "    // allocate memory on the device\n",
        "    cudaMalloc((void **)&d_a, size);\n",
        "    cudaMalloc((void **)&d_b, size);\n",
        "    cudaMalloc((void **)&d_c, size);\n",
        "\n",
        "    // copy inputs to device\n",
        "    cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // launch the kernel\n",
        "    dim3 threadsPerBlock(N, N);\n",
        "    dim3 blocksPerGrid(1, 1);\n",
        "    if (N*N > 512){\n",
        "        threadsPerBlock.x = 512;\n",
        "        threadsPerBlock.y = 512;\n",
        "        blocksPerGrid.x = ceil(double(N)/double(threadsPerBlock.x));\n",
        "        blocksPerGrid.y = ceil(double(N)/double(threadsPerBlock.y));\n",
        "    }\n",
        "\n",
        "    matrixMul<<<blocksPerGrid,threadsPerBlock>>>(d_a, d_b, d_c);\n",
        "\n",
        "    // copy result back to host\n",
        "    cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // print the result\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        for (int j = 0; j < N; j++) {\n",
        "            printf(\"%d \", c[i * N + j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    // free memory on the device\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jekwUKFte7k",
        "outputId": "99c570bd-941e-4a96-de79-287de0e25b35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80 70 60 50 \n",
            "240 214 188 162 \n",
            "400 358 316 274 \n",
            "560 502 444 386 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a program to multiply two matrices using CUDA."
      ],
      "metadata": {
        "id": "WaBArE9Xmjja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "\n",
        "#define M 2 // number of rows in the first matrix and the result matrix\n",
        "#define N 3 // number of columns in the first matrix and rows in the second matrix\n",
        "#define P 3 // number of columns in the second matrix and the result matrix\n",
        "\n",
        "__global__ void matrixMul(int *a, int *b, int *c) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    int temp_sum = 0;\n",
        "    if (row < M && col < P) {\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            temp_sum += a[row * N + i] * b[i * P + col];\n",
        "        }\n",
        "        c[row * P + col] = temp_sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int sizeA = M * N * sizeof(int);\n",
        "    int sizeB = N * P * sizeof(int);\n",
        "    int sizeC = M * P * sizeof(int);\n",
        "    int a[M*N] = {1, 1, 3, 0, 3, 8};\n",
        "    int b[N*P] = {1, 9, 6, 0, 1, 2};\n",
        "    int c[M*P];\n",
        "    int *d_a, *d_b, *d_c;\n",
        "\n",
        "    // allocate memory on the device\n",
        "    cudaMalloc((void **)&d_a, sizeA);\n",
        "    cudaMalloc((void **)&d_b, sizeB);\n",
        "    cudaMalloc((void **)&d_c, sizeC);\n",
        "\n",
        "    // copy inputs to device\n",
        "    cudaMemcpy(d_a, a, sizeA, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, b, sizeB, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // launch the kernel\n",
        "    dim3 threadsPerBlock(N, N);\n",
        "    dim3 blocksPerGrid(1, 1);\n",
        "    if (N*N > 512){\n",
        "        threadsPerBlock.x = 512;\n",
        "        threadsPerBlock.y = 512;\n",
        "        blocksPerGrid.x = ceil(double(N)/double(threadsPerBlock.x));\n",
        "        blocksPerGrid.y = ceil(double(N)/double(threadsPerBlock.y));\n",
        "    }\n",
        "\n",
        "    matrixMul<<<blocksPerGrid,threadsPerBlock>>>(d_a, d_b, d_c);\n",
        "\n",
        "    // copy result back to host\n",
        "    cudaMemcpy(c, d_c, sizeC, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // print the result\n",
        "    for (int i = 0; i < M; i++) {\n",
        "        for (int j = 0; j < P; j++) {\n",
        "            printf(\"%d \", c[i * P + j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    // free memory on the device\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eih6ITMte97",
        "outputId": "04d8c432-99d8-4bf6-d7be-5eb34dda413d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 10 8 \n",
            "0 3 6 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "//main program\n",
        "#include <stdio.h>\n",
        "\n",
        "#define M 3 // number of rows in the first matrix and the result matrix\n",
        "#define N 4 // number of columns in the first matrix and rows in the second matrix\n",
        "#define P 5 // number of columns in the second matrix and the result matrix\n",
        "\n",
        "__global__ void matrixMul(int *a, int *b, int *c) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    int temp_sum = 0;\n",
        "    if (row < M && col < P) {\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            temp_sum += a[row * N + i] * b[i * P + col];\n",
        "        }\n",
        "        c[row * P + col] = temp_sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int sizeA = M * N * sizeof(int);\n",
        "    int sizeB = N * P * sizeof(int);\n",
        "    int sizeC = M * P * sizeof(int);\n",
        "    int a[M*N] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12};\n",
        "    int b[N*P] = {1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4};\n",
        "    int c[M*P];\n",
        "    int *d_a, *d_b, *d_c;\n",
        "\n",
        "    // allocate memory on the device\n",
        "    cudaMalloc((void **)&d_a, sizeA);\n",
        "    cudaMalloc((void **)&d_b, sizeB);\n",
        "    cudaMalloc((void **)&d_c, sizeC);\n",
        "\n",
        "    // copy inputs to device\n",
        "    cudaMemcpy(d_a, a, sizeA, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, b, sizeB, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // launch the kernel\n",
        "    dim3 threadsPerBlock(16, 16); // Assuming a maximum of 256 threads per block\n",
        "    dim3 blocksPerGrid((P + threadsPerBlock.x - 1) / threadsPerBlock.x, (M + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
        "\n",
        "    matrixMul<<<blocksPerGrid, threadsPerBlock>>>(d_a, d_b, d_c);\n",
        "\n",
        "    // copy result back to host\n",
        "    cudaMemcpy(c, d_c, sizeC, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // print the result\n",
        "    for (int i = 0; i < M; i++) {\n",
        "        for (int j = 0; j < P; j++) {\n",
        "            printf(\"%d \", c[i * P + j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    // free memory on the device\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "Sa6S5czCvbeB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27643b17-d1f7-4b3d-d009-1826f985c969"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30 30 30 30 30 \n",
            "70 70 70 70 70 \n",
            "110 110 110 110 110 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Write a CUDA program to find the sum of elements of an array using parallel reduction."
      ],
      "metadata": {
        "id": "Z7qT86Tlm4r0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "const int N = 16; // Number of elements in the array\n",
        "\n",
        "__global__ void reduceSum(int* input, int* output, int n) {\n",
        "    extern __shared__ int sharedMem[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // Load data from global memory to shared memory\n",
        "    sharedMem[tid] = (idx < n) ? input[idx] : 0;\n",
        "    __syncthreads();\n",
        "\n",
        "    // Perform parallel reduction\n",
        "    for (int stride = 1; stride < blockDim.x; stride *= 2) {\n",
        "        int index = 2 * stride * tid;\n",
        "        if (index < blockDim.x) {\n",
        "            sharedMem[index] += sharedMem[index + stride];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Write the result to output array\n",
        "    if (tid == 0) {\n",
        "        output[blockIdx.x] = sharedMem[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int array[N] = {10, 1, 8, -1, 0, -2, 3, 5, -2, -3, 2, 7, 0, 11, 0, 2};\n",
        "    int* d_input, * d_output;\n",
        "\n",
        "    // Allocate memory on the device\n",
        "    cudaMalloc((void**)&d_input, N * sizeof(int));\n",
        "    cudaMalloc((void**)&d_output, sizeof(int));\n",
        "\n",
        "    // Copy the array from host to device\n",
        "    cudaMemcpy(d_input, array, N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Define grid and block sizes\n",
        "    dim3 gridSize(1, 1, 1);\n",
        "    dim3 blockSize(N, 1, 1);\n",
        "\n",
        "    // Timing variables\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    // Record the start event\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    // Launch the kernel\n",
        "    reduceSum<<<gridSize, blockSize, N * sizeof(int)>>>(d_input, d_output, N);\n",
        "\n",
        "    // Record the stop event\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    // Copy the result back to the host\n",
        "    int result;\n",
        "    cudaMemcpy(&result, d_output, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print the result\n",
        "    std::cout << \"Sum: \" << result << std::endl;\n",
        "\n",
        "     // Calculate and print the execution time\n",
        "    float milliseconds = 0;\n",
        "    cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "    std::cout << \"Execution Time: \" << milliseconds << \" ms\" << std::endl;\n",
        "\n",
        "    // Free allocated memory on the device\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output);\n",
        "\n",
        "    // Destroy events\n",
        "    cudaEventDestroy(start);\n",
        "    cudaEventDestroy(stop);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "heD7KBmZtfDG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d34eda0-89c0-4f23-a98d-c1b16ed77dfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum: 41\n",
            "Execution Time: 41.5871 ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "\n",
        "// CUDA kernel to perform reduction operation\n",
        "__global__ void sum(int *input, int *output, int n) {\n",
        "    extern __shared__ int shared[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (i < n) {\n",
        "        shared[tid] = input[i];\n",
        "    } else {\n",
        "        shared[tid] = 0;\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    for (int stride = 1; stride < blockDim.x; stride *= 2) {\n",
        "        if (tid % (2 * stride) == 0) {\n",
        "            shared[tid] += shared[tid + stride];\n",
        "        }\n",
        "\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid == 0) {\n",
        "        output[blockIdx.x] = shared[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "// Host function to initialize array elements and launch kernel\n",
        "int main() {\n",
        "    int n = 100;\n",
        "    int *input, *output;\n",
        "    int size = n * sizeof(int);\n",
        "\n",
        "    // Allocate memory on the host\n",
        "    input = (int*)malloc(size);\n",
        "    output = (int*)malloc(size);\n",
        "\n",
        "    // Initialize array elements\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        input[i] = i;\n",
        "    }\n",
        "\n",
        "    // Allocate memory on the device\n",
        "    int *d_input, *d_output;\n",
        "    cudaMalloc((void**)&d_input, size);\n",
        "    cudaMalloc((void**)&d_output, size);\n",
        "\n",
        "    // Copy input array to device\n",
        "    cudaMemcpy(d_input, input, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Timing variables\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    // Record the start event\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    // Launch kernel\n",
        "    int blockSize = 256;\n",
        "    int gridSize = (n + blockSize - 1) / blockSize;\n",
        "    sum<<<gridSize, blockSize, blockSize * sizeof(int)>>>(d_input, d_output, n);\n",
        "\n",
        "    // Record the stop event\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    // Copy result back to host\n",
        "    cudaMemcpy(output, d_output, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Calculate final sum\n",
        "    int final_sum = 0;\n",
        "    for (int i = 0; i < gridSize; i++) {\n",
        "        final_sum += output[i];\n",
        "    }\n",
        "\n",
        "    // Print final sum\n",
        "    printf(\"Sum: %d\\n\", final_sum);\n",
        "\n",
        "    // Calculate and print the execution time\n",
        "    float milliseconds = 0;\n",
        "    cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "    printf(\"Execution Time: %.6f ms\\n\", milliseconds);\n",
        "\n",
        "    // Free memory\n",
        "    free(input);\n",
        "    free(output);\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output);\n",
        "\n",
        "    // Destroy events\n",
        "    cudaEventDestroy(start);\n",
        "    cudaEventDestroy(stop);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "T9PA950OtfH9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80d84094-96aa-4f61-ae91-02fa340085ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sum: 4950\n",
            "Execution Time: 49.958080 ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Write a CUDA program to find the minimum element in an array using parallel reduction."
      ],
      "metadata": {
        "id": "BO1xRYDCpsiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <float.h>\n",
        "#include <sys/time.h>\n",
        "\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "// Kernel for parallel reduction\n",
        "__global__ void minParallelReduction(float *input, float *output, int n) {\n",
        "    __shared__ float sdata[BLOCK_SIZE];\n",
        "\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    sdata[tid] = (i < n) ? input[i] : FLT_MAX;\n",
        "    __syncthreads();\n",
        "\n",
        "    for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
        "        if (tid < s) {\n",
        "            sdata[tid] = fminf(sdata[tid], sdata[tid + s]);\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid == 0) {\n",
        "        output[blockIdx.x] = sdata[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "// Serial reduction on CPU\n",
        "float minSerialReduction(float *input, int n) {\n",
        "    float min_val = FLT_MAX;\n",
        "    for (int i = 0; i < n; ++i) {\n",
        "        min_val = fminf(min_val, input[i]);\n",
        "    }\n",
        "    return min_val;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = 7; // Size of the array\n",
        "    float h_input[] = {10, 21, 13, 41, 55, 76, 7};\n",
        "    float *d_input, *d_output;\n",
        "    float *h_output_parallel, *h_output_serial;\n",
        "    cudaEvent_t start, stop;\n",
        "    float elapsedTimeParallel, elapsedTimeSerial;\n",
        "\n",
        "    // Allocate host memory\n",
        "    h_output_parallel = (float *)malloc(sizeof(float));\n",
        "    h_output_serial = (float *)malloc(sizeof(float));\n",
        "\n",
        "    // Allocate device memory\n",
        "    cudaMalloc((void **)&d_input, n * sizeof(float));\n",
        "    cudaMalloc((void **)&d_output, sizeof(float));\n",
        "\n",
        "    // Copy host input array to device\n",
        "    cudaMemcpy(d_input, h_input, n * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Run parallel reduction\n",
        "    dim3 blockDim(BLOCK_SIZE, 1, 1);\n",
        "    dim3 gridDim((n + blockDim.x - 1) / blockDim.x, 1, 1);\n",
        "\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "    cudaEventRecord(start, 0);\n",
        "\n",
        "    minParallelReduction<<<gridDim, blockDim>>>(d_input, d_output, n);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    cudaEventRecord(stop, 0);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&elapsedTimeParallel, start, stop);\n",
        "\n",
        "    // Copy result back to host\n",
        "    cudaMemcpy(h_output_parallel, d_output, sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Run serial reduction\n",
        "    cudaEventRecord(start, 0);\n",
        "\n",
        "    *h_output_serial = minSerialReduction(h_input, n);\n",
        "\n",
        "    cudaEventRecord(stop, 0);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&elapsedTimeSerial, start, stop);\n",
        "\n",
        "    // Compare results\n",
        "    printf(\"Minimum value (serial reduction): %f\\n\", *h_output_parallel);\n",
        "    printf(\"Minimum value (parallel reduction): %f\\n\", *h_output_serial);\n",
        "\n",
        "    // Print execution times\n",
        "    printf(\"Execution time (serial reduction): %f ms\\n\", elapsedTimeParallel);\n",
        "    printf(\"Execution time (parallel reduction): %f ms\\n\", elapsedTimeSerial);\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output);\n",
        "\n",
        "    // Free host memory\n",
        "    free(h_output_parallel);\n",
        "    free(h_output_serial);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "ZUN_50ictfNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a CUDA program to implement 1D convolution."
      ],
      "metadata": {
        "id": "fMhqfSznpHXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "\n",
        "#define MASK_WIDTH 3\n",
        "#define DATA_SIZE 8\n",
        "#define TILE_WIDTH 4\n",
        "\n",
        "__constant__ int mask[MASK_WIDTH];\n",
        "\n",
        "__global__ void convolution_1d(int *input, int *output, int width) {\n",
        "    __shared__ int N_ds[TILE_WIDTH + MASK_WIDTH - 1];\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int temp = 0;\n",
        "\n",
        "    // Load input data into shared memory\n",
        "    if (i >= 0 && i < width)\n",
        "        N_ds[threadIdx.x + MASK_WIDTH/2] = input[i];\n",
        "    else\n",
        "        N_ds[threadIdx.x + MASK_WIDTH/2] = 0;\n",
        "\n",
        "    // Load halo elements\n",
        "    if (threadIdx.x < MASK_WIDTH/2) {\n",
        "        if (i - MASK_WIDTH/2 >= 0)\n",
        "            N_ds[threadIdx.x] = input[i - MASK_WIDTH/2];\n",
        "        else\n",
        "            N_ds[threadIdx.x] = 0;\n",
        "        if (i + blockDim.x < width)\n",
        "            N_ds[threadIdx.x + TILE_WIDTH + MASK_WIDTH/2] = input[i + blockDim.x];\n",
        "        else\n",
        "            N_ds[threadIdx.x + TILE_WIDTH + MASK_WIDTH/2] = 0;\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    // Convolution\n",
        "    for (int j = 0; j < MASK_WIDTH; j++)\n",
        "        temp += N_ds[threadIdx.x + j] * mask[j];\n",
        "\n",
        "    output[i] = temp;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int input[DATA_SIZE] = {1, 2, 3, 4, 5, 6, 7, 8};\n",
        "    int mask_value[MASK_WIDTH] = {1, 0, -1};\n",
        "    int output[DATA_SIZE] = {0};\n",
        "\n",
        "    int *d_input, *d_output;\n",
        "\n",
        "    cudaMalloc(&d_input, DATA_SIZE * sizeof(int));\n",
        "    cudaMalloc(&d_output, DATA_SIZE * sizeof(int));\n",
        "    cudaMemcpy(d_input, input, DATA_SIZE * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpyToSymbol(mask, mask_value, MASK_WIDTH * sizeof(int));\n",
        "\n",
        "    convolution_1d<<<1, DATA_SIZE>>>(d_input, d_output, DATA_SIZE);\n",
        "\n",
        "    cudaMemcpy(output, d_output, DATA_SIZE * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    for (int i = 0; i < DATA_SIZE; i++)\n",
        "        printf(\"%d \", output[i]);\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "kyGXmLGOtfPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a CUDA program to implement 2D convolution."
      ],
      "metadata": {
        "id": "spkNf597pKFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "\n",
        "#define MASK_WIDTH 3\n",
        "#define DATA_SIZE_X 5\n",
        "#define DATA_SIZE_Y 5\n",
        "#define TILE_WIDTH_X 3\n",
        "#define TILE_WIDTH_Y 3\n",
        "\n",
        "__constant__ int mask[MASK_WIDTH][MASK_WIDTH];\n",
        "\n",
        "__global__ void convolution_2d(int *input, int *output, int width, int height) {\n",
        "    __shared__ int N_ds[TILE_WIDTH_Y + MASK_WIDTH - 1][TILE_WIDTH_X + MASK_WIDTH - 1];\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int j = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int temp = 0;\n",
        "\n",
        "    // Load input data into shared memory\n",
        "    if (i >= 0 && i < width && j >= 0 && j < height)\n",
        "        N_ds[threadIdx.y + MASK_WIDTH/2][threadIdx.x + MASK_WIDTH/2] = input[j * width + i];\n",
        "    else\n",
        "        N_ds[threadIdx.y + MASK_WIDTH/2][threadIdx.x + MASK_WIDTH/2] = 0;\n",
        "\n",
        "    // Load halo elements\n",
        "    if (threadIdx.x < MASK_WIDTH/2) {\n",
        "        if (i - MASK_WIDTH/2 >= 0)\n",
        "            N_ds[threadIdx.y + MASK_WIDTH/2][threadIdx.x] = input[j * width + (i - MASK_WIDTH/2)];\n",
        "        else\n",
        "            N_ds[threadIdx.y + MASK_WIDTH/2][threadIdx.x] = 0;\n",
        "        if (i + blockDim.x < width)\n",
        "            N_ds[threadIdx.y + MASK_WIDTH/2][threadIdx.x + TILE_WIDTH_X + MASK_WIDTH/2] = input[j * width + (i + blockDim.x)];\n",
        "        else\n",
        "            N_ds[threadIdx.y + MASK_WIDTH/2][threadIdx.x + TILE_WIDTH_X + MASK_WIDTH/2] = 0;\n",
        "    }\n",
        "    if (threadIdx.y < MASK_WIDTH/2) {\n",
        "        if (j - MASK_WIDTH/2 >= 0)\n",
        "            N_ds[threadIdx.y][threadIdx.x + MASK_WIDTH/2] = input[(j - MASK_WIDTH/2) * width + i];\n",
        "        else\n",
        "            N_ds[threadIdx.y][threadIdx.x + MASK_WIDTH/2] = 0;\n",
        "        if (j + blockDim.y < height)\n",
        "            N_ds[threadIdx.y + TILE_WIDTH_Y + MASK_WIDTH/2][threadIdx.x + MASK_WIDTH/2] = input[(j + blockDim.y) * width + i];\n",
        "        else\n",
        "            N_ds[threadIdx.y + TILE_WIDTH_Y + MASK_WIDTH/2][threadIdx.x + MASK_WIDTH/2] = 0;\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    // Convolution\n",
        "    for (int y = 0; y < MASK_WIDTH; y++) {\n",
        "        for (int x = 0; x < MASK_WIDTH; x++) {\n",
        "            temp += N_ds[threadIdx.y + y][threadIdx.x + x] * mask[y][x];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if (i < width && j < height)\n",
        "        output[j * width + i] = temp;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int input[DATA_SIZE_Y][DATA_SIZE_X] = {\n",
        "        {1, 2, 3, 4, 5},\n",
        "        {6, 7, 8, 9, 10},\n",
        "        {11, 12, 13, 14, 15},\n",
        "        {16, 17, 18, 19, 20},\n",
        "        {21, 22, 23, 24, 25}\n",
        "    };\n",
        "    int mask_value[MASK_WIDTH][MASK_WIDTH] = {\n",
        "        {1, 0, -1},\n",
        "        {2, 0, -2},\n",
        "        {1, 0, -1}\n",
        "    };\n",
        "    int output[DATA_SIZE_Y][DATA_SIZE_X] = {0};\n",
        "\n",
        "    int *d_input, *d_output;\n",
        "\n",
        "    cudaMalloc(&d_input, DATA_SIZE_X * DATA_SIZE_Y * sizeof(int));\n",
        "    cudaMalloc(&d_output, DATA_SIZE_X * DATA_SIZE_Y * sizeof(int));\n",
        "    cudaMemcpy(d_input, input, DATA_SIZE_X * DATA_SIZE_Y * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpyToSymbol(mask, mask_value, MASK_WIDTH * MASK_WIDTH * sizeof(int));\n",
        "\n",
        "    dim3 dimBlock(TILE_WIDTH_X, TILE_WIDTH_Y);\n",
        "    dim3 dimGrid((DATA_SIZE_X + dimBlock.x - 1) / dimBlock.x, (DATA_SIZE_Y + dimBlock.y - 1) / dimBlock.y);\n",
        "\n",
        "    convolution_2d<<<dimGrid, dimBlock>>>(d_input, d_output, DATA_SIZE_X, DATA_SIZE_Y);\n",
        "\n",
        "    cudaMemcpy(output, d_output, DATA_SIZE_X * DATA_SIZE_Y * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    for (int j = 0; j < DATA_SIZE_Y; j++) {\n",
        "        for (int i = 0; i < DATA_SIZE_X; i++)\n",
        "            printf(\"%d \", output[j][i]);\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "xorK6IdqJUh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iepknbh9JUs_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}